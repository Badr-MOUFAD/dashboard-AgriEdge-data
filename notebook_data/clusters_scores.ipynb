{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# change the working directory to point to the root directory\r\n",
    "import os\r\n",
    "\r\n",
    "os.chdir(\"../\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# imports\r\n",
    "import numpy as np\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# imports for plots\r\n",
    "from plotly import graph_objects as go\r\n",
    "import plotly.express as px\r\n",
    "from plotly.subplots import make_subplots\r\n",
    "import plotly.io as pio\r\n",
    "\r\n",
    "pio.templates.default = \"plotly_white\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# structure of dict\r\n",
    "{ \r\n",
    "    \"univariate\": {\r\n",
    "        \"cumulative_PRECTOT\": { \"nb_clusters\": 0, \"score\": 0, \"clusters\": {} },\r\n",
    "        \"cumulative_PRECTOT\": { \"nb_clusters\": 0, \"score\": 0, \"clusters\": {} },\r\n",
    "        \"cumulative_PRECTOT\": { \"nb_clusters\": 0, \"score\": 0, \"clusters\": {} },\r\n",
    "        \"cumulative_PRECTOT\": { \"nb_clusters\": 0, \"score\": 0, \"clusters\": {} },\r\n",
    "    },\r\n",
    "    \"multivariate\": { \"nb_clusters\": 0, \"score\": 0, \"clusters\": {} }\r\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'univariate': {'cumulative_PRECTOT': {'nb_clusters': 0,\n",
       "   'score': 0,\n",
       "   'clusters': {}}},\n",
       " 'multivariate': {'nb_clusters': 0, 'score': 0, 'clusters': {}}}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# load data\r\n",
    "data_region = pd.read_csv(\"raw_data/weather_regions/33.79102828115297;-3.67179828879979;Taza-Al Hoceima-Taounate;Guercif.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# imports and general params\r\n",
    "from py_scripts.clustering import isolate_data_col\r\n",
    "from py_scripts.clustering import reduce_merge_multivariate_data, multivariate_clustering, univariate_clustering\r\n",
    "from py_scripts.clustering import get_best_number_clusters\r\n",
    "\r\n",
    "selected_cols = [\"cumulative_GDD\", \"cumulative_PRECTOT\", \"cumulative_WS2M\", \"cumulative_RH2M\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def generate_info_region(data_region):\r\n",
    "    # univariate\r\n",
    "    dict_uni_var = {}\r\n",
    "    for col in selected_cols:\r\n",
    "        # construct data\r\n",
    "        data_var = isolate_data_col(data=data_region, col_name=col)\r\n",
    "\r\n",
    "        # where to store info of var\r\n",
    "        dict_var = { \"clusters\": {}}\r\n",
    "\r\n",
    "        # nb clusters and corresponding score\r\n",
    "        dict_var[\"nb_clusters\"], dict_var[\"score\"] = max(list(get_best_number_clusters(data_var).items()), key=lambda x: x[1])\r\n",
    "\r\n",
    "        # cluster data (use entire data region)\r\n",
    "        dict_var[\"clusters\"] = univariate_clustering(data_region, col_name=col, nb_clusters=dict_var[\"nb_clusters\"])\r\n",
    "\r\n",
    "        # save result clustering of col\r\n",
    "        dict_uni_var[col] = dict_var\r\n",
    "\r\n",
    "    # multivariate\r\n",
    "    dict_multi_var = {}\r\n",
    "    reduced_data = reduce_merge_multivariate_data({ col: isolate_data_col(data_region, col) for col in selected_cols})\r\n",
    "\r\n",
    "    # ideal nb clusters and score\r\n",
    "    dict_multi_var[\"nb_clusters\"], dict_multi_var[\"score\"] = max(list(get_best_number_clusters(reduced_data).items()), key=lambda x: x[1])\r\n",
    "\r\n",
    "    # multi clustering clustering\r\n",
    "    dict_multi_var[\"clusters\"] = multivariate_clustering(data_region, nb_clusters=dict_multi_var[\"nb_clusters\"])\r\n",
    "\r\n",
    "    return { \"univariate\": dict_uni_var, \"multivariate\": dict_multi_var }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# get dict info from data region\r\n",
    "dict_all_infos = generate_info_region(data_region)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dict_all_infos[\"multivariate\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'nb_clusters': 2,\n",
       " 'score': 36.13377939286226,\n",
       " 'clusters': {0: Int64Index([1991, 1994, 1996, 2001, 2004, 2006, 2009, 2010, 2011, 2012, 2013,\n",
       "              2015, 2018],\n",
       "             dtype='int64'),\n",
       "  1: Int64Index([1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1992, 1993,\n",
       "              1995, 1997, 1998, 1999, 2000, 2002, 2003, 2005, 2007, 2008, 2014,\n",
       "              2016, 2017, 2019, 2020],\n",
       "             dtype='int64')}}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Save\r\n",
    "np.save('processed_data/test_region.npy', dict_all_infos)\r\n",
    "\r\n",
    "# Load\r\n",
    "read_dictionary = np.load('processed_data/test_region.npy', allow_pickle='TRUE').item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "max([(key, val['nb_clusters'], val[\"score\"]) for key, val in dict_all_infos[\"univariate\"].items()], key=lambda x: x[2])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('cumulative_RH2M', 5, 54.43284969426086)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# # generate infos of all regions\r\n",
    "\r\n",
    "# dir_path = \"raw_data/weather_regions/\"\r\n",
    "# for file in os.listdir(dir_path):\r\n",
    "#     # how dict will be named: remove extension (.csv) from file\r\n",
    "#     filename = file[:-4]\r\n",
    "\r\n",
    "#     # load data region\r\n",
    "#     data_region = pd.read_csv(dir_path + file)\r\n",
    "\r\n",
    "#     # get all infos\r\n",
    "#     dict_all_infos = generate_info_region(data_region)\r\n",
    "\r\n",
    "#     # save dict\r\n",
    "#     np.save(f\"processed_data/individual_regions/{filename}.npy\", dict_all_infos)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "1e915f0a29dc84041eaeb02b7b1a21c440e37a87b61d44d5e84a515737dc82bc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}